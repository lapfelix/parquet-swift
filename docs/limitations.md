# Known Limitations - Phase 1

This document tracks known limitations and compatibility issues in the current implementation.

## Parquet Metadata Format Compatibility

### PyArrow-Generated Files (BLOCKED)

**Status**: ❌ Not supported in Phase 1

**Issue**: Thrift metadata parser cannot read files generated by PyArrow (`parquet-cpp-arrow`).

**Symptoms**:
- Error: `"Missing required fields in FileMetaData"`
- Metadata parsing stops after reading `version` and `schema` fields
- Missing: `numRows` and `rowGroups` fields

**Affected Files**:
- Any Parquet file with `created_by: "parquet-cpp-arrow version X.X.X"`
- Tested with PyArrow 21.0.0

**Root Cause**: Unknown - requires investigation of:
- Thrift Compact Binary encoding differences
- Schema element list parsing (may consume extra bytes)
- Optional field handling differences
- Field ordering assumptions

**Workaround**: Use parquet-mr generated files:
```
created_by: "parquet-mr version X.X.X"
```

**Files Known to Work**:
- `datapage_v1-snappy-compressed-checksum.parquet` (parquet-mr 1.13.0)
- Any file from Apache Spark, Hive, or parquet-mr tools

### Future Work
- [ ] Debug schema list parsing in `ThriftReader+FileMetaData.swift`
- [ ] Add PyArrow compatibility tests
- [ ] Test with multiple PyArrow versions
- [ ] Consider alternative Thrift implementations

## Encoding Support

### Dictionary Encoding (BLOCKED)

**Status**: ❌ Not supported in Phase 1

**Affected Encodings**:
- `PLAIN_DICTIONARY` (deprecated)
- `RLE_DICTIONARY`

**Workaround**: Use PLAIN encoding only
- parquet-mr: Set `WriterVersion.PARQUET_1_0`, disable dictionary
- PyArrow: `use_dictionary=False` (but see PyArrow limitation above)

**Impact**: Most real-world Parquet files use dictionary encoding for strings and low-cardinality columns, making them unreadable in Phase 1.

## Compression Support

### Snappy Compression (NOT IMPLEMENTED)

**Status**: ❌ Not implemented in Phase 1

**Supported Codecs**:
- ✅ UNCOMPRESSED
- ✅ GZIP

**Unsupported Codecs**:
- ❌ SNAPPY (most common in practice)
- ❌ LZ4
- ❌ LZ4_RAW
- ❌ ZSTD
- ❌ BROTLI
- ❌ LZO

**Impact**: Many production Parquet files use Snappy compression (default in Spark), making them unreadable in Phase 1.

## Test Fixtures

### No Suitable Test Fixture for Column Readers

**Status**: ❌ Blocks end-to-end testing

**Requirements for Phase 1 Testing**:
1. PLAIN encoding only (no dictionary)
2. UNCOMPRESSED or GZIP compression (not Snappy)
3. parquet-mr generated (not PyArrow)
4. Multiple column types: Int32, Int64, Float, Double, String

**Existing Fixtures - Why They Don't Work**:
- `alltypes_plain.parquet`: Uses dictionary encoding ❌
- `datapage_v1-snappy-compressed-checksum.parquet`: Uses Snappy ❌
- `plain_types.parquet`: PyArrow-generated, metadata parse fails ❌

**Solution**: Generate test fixture using parquet-mr Java tools:
```java
// Pseudocode - need actual parquet-mr example
ParquetWriter.builder(outputFile)
    .withWriterVersion(WriterVersion.PARQUET_1_0)
    .withDictionaryEncoding(false)
    .withCompressionCodec(CompressionCodecName.GZIP)
    .build();
```

See `generate_plain_fixture.py` for reference (PyArrow version that doesn't work).

## Column Features

### Nullable Columns (NOT IMPLEMENTED)

**Status**: ❌ Not implemented in Phase 1

**Missing**: Definition level / repetition level support

**Impact**: Can only read required (non-nullable) columns. Most real Parquet schemas have nullable columns.

### Nested Types (NOT IMPLEMENTED)

**Status**: ❌ Not implemented in Phase 1

**Missing Support**:
- Nested structs
- Lists/arrays
- Maps

**Impact**: Can only read flat, primitive columns.

## Summary

Phase 1 implementation supports a **minimal subset** of Parquet:
- ✅ parquet-mr generated files
- ✅ PLAIN encoding
- ✅ UNCOMPRESSED or GZIP compression
- ✅ Required (non-nullable) primitive columns
- ✅ Types: Int32, Int64, Float, Double, String

This covers basic read scenarios but **does not work** with most real-world Parquet files, which typically use:
- Dictionary encoding (strings, enums)
- Snappy compression (Spark default)
- Nullable columns (definition levels)
- PyArrow tools (Python ecosystem)

Phase 2+ roadmap should prioritize:
1. **Snappy compression** (unblocks 80% of files)
2. **Dictionary encoding** (unblocks string columns)
3. **PyArrow compatibility** (unblocks Python ecosystem)
4. **Definition levels** (nullable columns)
