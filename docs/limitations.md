# Known Limitations - Phase 1

This document tracks known limitations and compatibility issues in the current implementation.

## Parquet Metadata Format Compatibility

### PyArrow-Generated Files (BLOCKED)

**Status**: ❌ Not supported in Phase 1

**Issue**: Thrift metadata parser cannot read files generated by PyArrow (`parquet-cpp-arrow`).

**Symptoms**:
- Error: `"Missing required fields in FileMetaData"`
- Metadata parsing stops after reading `version` and `schema` fields
- Missing: `numRows` and `rowGroups` fields

**Affected Files**:
- Any Parquet file with `created_by: "parquet-cpp-arrow version X.X.X"`
- Tested with PyArrow 21.0.0

**Root Cause**: Unknown - requires investigation of:
- Thrift Compact Binary encoding differences
- Schema element list parsing (may consume extra bytes)
- Optional field handling differences
- Field ordering assumptions

**Workaround**: Use parquet-mr generated files:
```
created_by: "parquet-mr version X.X.X"
```

**Files Known to Work**:
- `datapage_v1-snappy-compressed-checksum.parquet` (parquet-mr 1.13.0)
- Any file from Apache Spark, Hive, or parquet-mr tools

### Future Work
- [ ] Debug schema list parsing in `ThriftReader+FileMetaData.swift`
- [ ] Add PyArrow compatibility tests
- [ ] Test with multiple PyArrow versions
- [ ] Consider alternative Thrift implementations

## Encoding Support

### Dictionary Encoding (BLOCKED)

**Status**: ❌ Not supported in Phase 1

**Affected Encodings**:
- `PLAIN_DICTIONARY` (deprecated)
- `RLE_DICTIONARY`

**Workaround**: Use PLAIN encoding only
- parquet-mr: Set `WriterVersion.PARQUET_1_0`, disable dictionary
- PyArrow: `use_dictionary=False` (but see PyArrow limitation above)

**Impact**: Most real-world Parquet files use dictionary encoding for strings and low-cardinality columns, making them unreadable in Phase 1.

## Compression Support

### Snappy Compression (IMPLEMENTED ✅)

**Status**: ✅ Implemented in Phase 2 (M2.0) - Pure Swift!

**Supported Codecs**:
- ✅ UNCOMPRESSED
- ✅ GZIP
- ✅ **SNAPPY** (most common in production) - Pure Swift implementation!

**Unsupported Codecs**:
- ❌ LZ4
- ❌ LZ4_RAW
- ❌ ZSTD
- ❌ BROTLI
- ❌ LZO

**Implementation**: Uses [snappy-swift](https://github.com/codelynx/snappy-swift), a pure Swift implementation with:
- ✅ **Zero system dependencies** - no brew/apt installation required
- ✅ **100% C++ compatible** - verified against Google's reference implementation
- ✅ **Fast performance** - 64-128 MB/s compression, 203-261 MB/s decompression
- ✅ **Cross-platform** - works on macOS, iOS, Linux, and all Swift platforms

**Build**: Simple `swift build` - no environment variables needed!

**Impact**: Most production Parquet files now readable! Snappy is the default compression in Apache Spark.

## Test Fixtures

### No Suitable Test Fixture for Column Readers

**Status**: ❌ Blocks end-to-end testing

**Requirements for Phase 2 Testing**:
1. PLAIN encoding only (no dictionary)
2. UNCOMPRESSED, GZIP, or Snappy compression ✅
3. parquet-mr generated (not PyArrow)
4. Multiple column types: Int32, Int64, Float, Double, String

**Existing Fixtures - Why They Don't Work**:
- `alltypes_plain.parquet`: Uses dictionary encoding ❌
- `datapage_v1-snappy-compressed-checksum.parquet`: Uses Snappy ✅ (NOW WORKS!)
- `plain_types.parquet`: PyArrow-generated, metadata parse fails ❌

**Solution**: Generate test fixture using parquet-mr Java tools:
```java
// Pseudocode - need actual parquet-mr example
ParquetWriter.builder(outputFile)
    .withWriterVersion(WriterVersion.PARQUET_1_0)
    .withDictionaryEncoding(false)
    .withCompressionCodec(CompressionCodecName.GZIP)
    .build();
```

See `generate_plain_fixture.py` for reference (PyArrow version that doesn't work).

## Column Features

### Nullable Columns (NOT IMPLEMENTED)

**Status**: ❌ Not implemented in Phase 1

**Missing**: Definition level / repetition level support

**Impact**: Can only read required (non-nullable) columns. Most real Parquet schemas have nullable columns.

### Nested Types (NOT IMPLEMENTED)

**Status**: ❌ Not implemented in Phase 1

**Missing Support**:
- Nested structs
- Lists/arrays
- Maps

**Impact**: Can only read flat, primitive columns.

## Summary

Phase 2.1 implementation supports:
- ✅ parquet-mr generated files
- ✅ PLAIN encoding
- ✅ **Dictionary encoding (RLE_DICTIONARY, PLAIN_DICTIONARY)** ✨ NEW!
- ✅ UNCOMPRESSED, GZIP, and **Snappy** compression
- ✅ **Required (non-nullable)** primitive columns
- ✅ Types: Int32 (others coming soon)

**Major Improvements**:
- ✅ Snappy compression support (~80% of production files)
- ✅ **Dictionary encoding for required columns** (~80% of string columns when combined with required constraint)

### Dictionary Encoding - Phase 2.1 Scope

**What works:**
- ✅ Required columns with dictionary encoding (Int32 currently)
- ✅ Both RLE_DICTIONARY and PLAIN_DICTIONARY encodings
- ✅ Full overflow protection in RLE decoder
- ✅ Strict byte-exact validation

**What doesn't work yet:**
- ❌ **Nullable columns** (maxDefinitionLevel > 0) - Phase 3
- ❌ **Repeated columns** (maxRepetitionLevel > 0) - Phase 3
- ❌ Other types (Int64, Float, Double, String) - Phase 2.2

**Why the limitation?**

Nullable and repeated columns encode definition/repetition levels before the data in each page.
Phase 2.1 skips this level decoding step, so it only works for required columns where no
level streams are present.

Still **does not work** with:
- Nullable columns (definition levels) - **Phase 3**
- Repeated columns (repetition levels) - **Phase 3**
- PyArrow tools (Python ecosystem) - metadata parsing incompatibility
- Nested types (lists, maps, structs) - Phase 4+

Remaining Phase 2 priorities:
1. ✅ **Dictionary encoding for required columns** (DONE - Phase 2.1)
2. **Extend dictionary encoding to all types** (Int64, Float, Double, String) - Phase 2.2
3. **Definition levels** (nullable columns) - Phase 3
4. **Repetition levels** (repeated columns) - Phase 3
5. **PyArrow compatibility** (unblocks Python ecosystem) - TBD
6. **Nested types** (lists, maps, structs) - Phase 4+
