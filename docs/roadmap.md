# Parquet-Swift Roadmap

**Last Updated**: 2025-11-04
**Current Status**: Phase 5 Complete - Production-Ready Reader

---

## Executive Summary

Parquet-Swift has achieved **full reader parity** with Apache Arrow C++ for virtually all real-world Parquet files. With Phase 5 complete, the reader supports:

- âœ… All primitive types with nullable and required fields
- âœ… Dictionary encoding and Snappy compression
- âœ… Full nested type support (lists, maps, structs with arbitrary nesting)
- âœ… PyArrow and parquet-mr file compatibility

**What's Next**: Writer implementation, performance optimizations, and advanced features.

---

## Phase Completion Status

### âœ… Phase 1: Practical Reader (COMPLETE)
**Duration**: 10 weeks
**Deliverable**: Alpha release

**Achievements**:
- Core type system and schema representation
- Thrift Compact Binary Protocol parser
- PLAIN encoding for all primitive types
- GZIP + UNCOMPRESSED codecs
- Instance-based file reader API
- Metadata wrapper API (idiomatic Swift)

### âœ… Phase 2: Full Encoding & Compression (COMPLETE)
**Duration**: 4 weeks
**Deliverable**: Beta release candidate

**Achievements**:
- M2.0: Snappy compression (pure Swift)
- M2.1: Dictionary encoding for Int32
- M2.2: Dictionary encoding for all primitive types

### âœ… Phase 3: Nullable & Nested Data (COMPLETE)
**Duration**: 6 weeks
**Deliverable**: Advanced reader features

**Achievements**:
- Definition levels (nullable columns)
- Repetition levels (array reconstruction)
- Multi-level nested lists
- PyArrow compatibility fixes (Thrift parser bugs)
- Single-level repeated columns

### âœ… Phase 4: Structs & Maps (COMPLETE)
**Duration**: 4 weeks
**Deliverable**: Complex type support

**Achievements**:
- Root-level struct reading
- Root-level map reading
- Nested structs (struct in struct)
- Nullable structs and maps
- LevelInfo infrastructure

### âœ… Phase 4.5: Structs with Complex Children (COMPLETE)
**Duration**: 2 weeks
**Deliverable**: Struct child reconstruction

**Achievements**:
- DefRepLevelsToBitmap for struct validity
- Child array reconstruction (maps, lists, scalars)
- Arrow C++ StructReader::BuildArray pattern
- Map key type preservation (AnyHashable)

### âœ… Phase 5: Lists of Complex Structs (COMPLETE)
**Duration**: 2 weeks
**Deliverable**: Full nested type parity

**Achievements**:
- computeStructRanges() for struct boundary detection
- Range-based child readers (map, list, repeated scalar, scalar)
- Complete column coverage
- Bounded reads aligned to struct boundaries
- NULL handling at all nesting levels
- Maps with list values: `map<K, list<V>>`
- Lists of structs with maps: `list<struct { map<K,V> }>`
- Lists of structs with lists: `list<struct { list<T> }>`

**Real-World Impact**: Can read virtually all production Parquet files with nested structures!

---

## Remaining Reader Features

### Phase 6: Additional Encodings (OPTIONAL)
**Priority**: Low
**Estimated Duration**: 2-3 weeks
**Complexity**: Medium

**Features**:
1. **Delta encodings**:
   - DELTA_BINARY_PACKED (integers)
   - DELTA_LENGTH_BYTE_ARRAY (strings)
   - DELTA_BYTE_ARRAY (strings with prefix compression)

2. **RLE encoding for booleans**:
   - Common for boolean columns
   - Requires bit-packing logic

3. **BYTE_STREAM_SPLIT encoding**:
   - Improves compression for floating-point data
   - Used in some scientific datasets

**Why Low Priority**:
- PLAIN + Dictionary covers 95%+ of real-world files
- Delta encodings mostly used in specialized data pipelines
- Can be added on-demand if users need specific encodings

**Timeline**: Add if user requests come in, not blocking 1.0

---

### Phase 7: Additional Compression Codecs (OPTIONAL)
**Priority**: Low
**Estimated Duration**: 2-3 weeks
**Complexity**: Low-Medium

**Features**:
1. **LZ4** / **LZ4_RAW**:
   - Fast compression, used in some Spark configurations
   - Pure Swift implementation available

2. **ZSTD** (Zstandard):
   - Modern codec, good compression ratios
   - Used in newer data platforms

3. **BROTLI**:
   - Web-optimized codec
   - Rare in Parquet files

**Why Low Priority**:
- Snappy covers ~80% of production files
- GZIP covers another 15%
- LZ4/ZSTD are nice-to-haves, not critical

**Timeline**: Add when SPM packages are stable, not blocking 1.0

---

### Phase 8: Data Page V2 (OPTIONAL)
**Priority**: Low
**Estimated Duration**: 2 weeks
**Complexity**: Medium

**Features**:
- Support reading Data Page V2 format
- Improved level encoding (no length prefix for rep/def levels)
- Better compression ratios

**Why Low Priority**:
- Page V1 is still dominant in production
- Page V2 adoption is growing but not critical yet
- Reader already handles 99%+ of files

**Timeline**: Add if V2 adoption increases, monitor Parquet ecosystem

---

### Phase 9: Deeply Nested Combinations (FUTURE)
**Priority**: Very Low
**Estimated Duration**: 4-6 weeks
**Complexity**: High

**Features**:
- `list<map<string, list<struct>>>` and similar patterns
- Arbitrary nesting depth
- Full LevelInfo port from Arrow C++

**Why Very Low Priority**:
- Extremely rare in real-world schemas
- Phase 5 covers 99.9%+ of use cases
- Significant implementation complexity for minimal gain

**Timeline**: Only if specific user requests emerge

---

## Writer Implementation

### Phase 10: Core Writer (HIGH PRIORITY)
**Priority**: High
**Estimated Duration**: 8-10 weeks
**Complexity**: High
**Target**: 1.0 Release

**Milestone W1: Basic File Writer (3 weeks)**
- File creation and metadata writing
- Schema serialization to Thrift
- Row group writer API
- Column writer interface
- PLAIN encoding for primitive types
- UNCOMPRESSED codec

**Deliverable**: Can write simple flat Parquet files

**Milestone W2: Compression & Encoding (2 weeks)**
- Snappy compression for writing
- GZIP compression for writing
- Dictionary encoding for writing (string columns)
- Statistics generation (min/max/null_count)

**Deliverable**: Can write production-quality files with compression

**Milestone W3: Nullable Columns (2 weeks)**
- Definition level encoding (RLE/bit-packed hybrid)
- Nullable column writing
- Correct level computation

**Deliverable**: Can write files with optional columns

**Milestone W4: Nested Types (3-4 weeks)**
- Repetition level encoding
- List writing
- Struct writing
- Map writing
- Nested structure level computation

**Deliverable**: Can write complex nested schemas

**Why High Priority**:
- Unlocks write use cases for iOS/macOS/Linux apps
- Needed for 1.0 release
- Many users want to generate Parquet files, not just read them

**API Design Preview**:
```swift
// Create file writer
let writer = try ParquetFileWriter(url: outputURL)
defer { try? writer.close() }

// Define schema
let schema = try SchemaBuilder()
    .addInt32Column("id", required: true)
    .addStringColumn("name", required: false)
    .build()

writer.setSchema(schema)

// Create row group
let rowGroup = try writer.createRowGroup()

// Write columns
let idWriter = try rowGroup.int32ColumnWriter(at: 0)
try idWriter.writeValues([1, 2, 3, 4, 5])

let nameWriter = try rowGroup.stringColumnWriter(at: 1)
try nameWriter.writeValues(["Alice", nil, "Charlie", "David", nil])

// Finalize
try writer.finalize()
```

---

### Phase 11: Advanced Writer Features (MEDIUM PRIORITY)
**Priority**: Medium
**Estimated Duration**: 4-6 weeks
**Complexity**: Medium

**Features**:
1. **Column statistics**:
   - Accurate min/max computation
   - Distinct value counts
   - Null counts

2. **Multiple row groups**:
   - Configurable row group size
   - Memory-efficient batch writing

3. **Dictionary encoding optimization**:
   - Adaptive dictionary vs PLAIN fallback
   - Dictionary page size tuning

4. **Bloom filters (writing)**:
   - Generate Bloom filters for columns
   - Configurable false positive rate

**Why Medium Priority**:
- Improves query performance on written files
- Not blocking basic writer functionality
- Can be added incrementally post-1.0

---

## Performance Optimizations

### Phase 12: Reader Performance (MEDIUM PRIORITY)
**Priority**: Medium
**Estimated Duration**: 4-6 weeks
**Complexity**: Medium-High

**Features**:
1. **Vectorized decoding**:
   - SIMD operations for PLAIN decoding
   - Batch level decoding
   - Reduced per-element overhead

2. **Memory pooling**:
   - Reuse buffers across page reads
   - Reduce allocations in hot paths

3. **Async I/O**:
   - Swift Concurrency support
   - Parallel column reading
   - Non-blocking file operations

4. **Zero-copy optimizations**:
   - Direct buffer access where possible
   - Reduce data copying

**Why Medium Priority**:
- Current reader is "fast enough" for most use cases
- Optimization is easier with working implementation
- Profiling should guide optimization work

**Benchmark Goals**:
- Match Arrow C++ within 2x for common operations
- < 1ms per row for typical queries
- Support files > 1GB without memory issues

---

### Phase 13: Writer Performance (LOW-MEDIUM PRIORITY)
**Priority**: Low-Medium
**Estimated Duration**: 3-4 weeks
**Complexity**: Medium

**Features**:
1. **Vectorized encoding**:
   - SIMD for PLAIN encoding
   - Batch dictionary encoding

2. **Parallel compression**:
   - Compress pages in parallel
   - Async row group writing

3. **Memory-efficient writing**:
   - Stream large datasets
   - Configurable buffer sizes

**Why Low-Medium Priority**:
- Get writer working first, then optimize
- Most write workloads aren't latency-sensitive
- Can add post-1.0 based on user feedback

---

## Advanced Features

### Phase 14: Bloom Filters (LOW PRIORITY)
**Priority**: Low
**Estimated Duration**: 2 weeks
**Complexity**: Low-Medium

**Features**:
- Read Bloom filters from files
- Use for query pruning
- Generate Bloom filters when writing

**Why Low Priority**:
- Not widely adopted in Parquet ecosystem yet
- Requires format spec understanding
- Nice-to-have for query optimization

---

### Phase 15: Page Index (LOW PRIORITY)
**Priority**: Low
**Estimated Duration**: 2-3 weeks
**Complexity**: Medium

**Features**:
- Read column index (page statistics)
- Read offset index (page locations)
- Use for row group/page pruning
- Generate indexes when writing

**Why Low Priority**:
- Advanced optimization feature
- Requires careful implementation
- Most tools don't use it yet

---

### Phase 16: Column Encryption (FUTURE)
**Priority**: Very Low
**Estimated Duration**: 4-6 weeks
**Complexity**: High

**Features**:
- Read encrypted columns
- Write encrypted columns
- Key management integration
- Modular encryption spec compliance

**Why Very Low Priority**:
- Rarely used in practice
- Complex spec with many edge cases
- Security-sensitive code requires extensive testing

---

## Ecosystem Integration

### Phase 17: Swift Concurrency Support (HIGH PRIORITY)
**Priority**: High
**Estimated Duration**: 3-4 weeks
**Complexity**: Medium
**Target**: 1.0 or 1.1

**Features**:
1. **Async/await API**:
   ```swift
   let reader = try await ParquetFileReader(url: fileURL)
   let values = try await column.readAll()
   ```

2. **Parallel column reading**:
   - Read multiple columns concurrently
   - TaskGroup-based parallelism

3. **Streaming API**:
   - AsyncSequence for large files
   - Memory-efficient iteration

**Why High Priority**:
- Modern Swift apps expect async APIs
- Enables better performance through parallelism
- Required for iOS/macOS best practices

---

### Phase 18: Platform Expansion (MEDIUM PRIORITY)
**Priority**: Medium
**Estimated Duration**: 2-3 weeks
**Complexity**: Low-Medium

**Features**:
1. **Linux testing & CI**:
   - GitHub Actions Linux builds
   - Swift 5.9+ on Ubuntu

2. **Windows support** (future):
   - Windows filesystem compatibility
   - File path handling

3. **WASM support** (future):
   - SwiftWasm compatibility
   - Browser-based Parquet reading

**Why Medium Priority**:
- Linux is important for server-side Swift
- Windows/WASM are nice-to-haves
- Platform tests can be added incrementally

---

### Phase 19: Data Frame Integration (FUTURE)
**Priority**: Low
**Estimated Duration**: 6-8 weeks
**Complexity**: High

**Features**:
- Integration with Swift DataFrame libraries
- Zero-copy data ingestion where possible
- Columnar memory layout compatibility

**Why Low Priority**:
- Swift DataFrame ecosystem is still emerging
- Requires coordination with other projects
- Can be external package, not core library

---

## Release Timeline

### Version 0.9 - Beta (Current)
**Target**: Now
**Status**: Phase 5 complete

**Features**:
- âœ… Full reader implementation
- âœ… All nested types supported
- âœ… Dictionary encoding + Snappy
- âœ… PyArrow compatibility

**Deliverable**: Production-ready reader library

---

### Version 1.0 - Full Release
**Target**: Q2 2025 (3-4 months)
**Prerequisites**: Writer implementation complete

**Features**:
- âœ… Complete reader (from Phase 5)
- âœ… Core writer (Phase 10)
- âœ… Swift Concurrency support (Phase 17)
- âœ… Comprehensive documentation
- âœ… Production test coverage
- âœ… Linux CI/CD

**Deliverable**: Full-featured Parquet library for Swift

---

### Version 1.1 - Performance & Polish
**Target**: Q3 2025 (2-3 months after 1.0)

**Features**:
- Reader performance optimizations (Phase 12)
- Writer performance optimizations (Phase 13)
- Additional encodings if requested (Phase 6)
- Additional codecs if requested (Phase 7)

**Deliverable**: Optimized production library

---

### Version 1.2+ - Advanced Features
**Target**: Q4 2025 and beyond

**Features**:
- Bloom filters (Phase 14)
- Page index (Phase 15)
- Data Page V2 (Phase 8)
- Deeply nested patterns (Phase 9)
- Platform expansion (Phase 18)

**Deliverable**: Feature-complete library matching Apache Arrow C++

---

## Recommended Priority Order

### Immediate (Next 3-4 months)
1. **Phase 10: Core Writer** - Critical for 1.0
2. **Phase 17: Swift Concurrency** - Modern API requirement
3. **Platform testing (Linux)** - Production readiness

### Short-term (4-8 months)
4. **Phase 11: Advanced Writer** - Post-1.0 polish
5. **Phase 12: Reader Performance** - Optimize based on profiling
6. **Additional encodings/codecs** - If user requests emerge

### Long-term (8+ months)
7. **Phase 14-15: Bloom filters, Page index** - Advanced query optimization
8. **Phase 18: Platform expansion** - Windows, WASM
9. **Phase 8: Data Page V2** - If ecosystem adoption grows
10. **Phase 19: DataFrame integration** - Ecosystem maturity dependent

---

## Success Metrics

### Reader (Current - Phase 5)
- âœ… **File compatibility**: 99%+ of real-world Parquet files readable
- âœ… **Type coverage**: All primitive types + full nested types
- âœ… **Encoding coverage**: PLAIN + Dictionary (95%+ of files)
- âœ… **Compression coverage**: Snappy + GZIP (95%+ of files)
- âœ… **Test coverage**: 352 tests passing, comprehensive fixture suite

### Writer (Target - Phase 10)
- **File compatibility**: Write files readable by Arrow C++, PyArrow, Spark
- **Type coverage**: All primitive types + nested types
- **Encoding coverage**: PLAIN + Dictionary for strings
- **Compression coverage**: Snappy + GZIP + UNCOMPRESSED
- **Test coverage**: 200+ writer-specific tests

### Performance (Target - Phase 12)
- **Read throughput**: Within 2x of Arrow C++ for common operations
- **Memory efficiency**: < 2x file size peak memory for typical workloads
- **Concurrency**: Linear speedup with parallel column reading

---

## Community & Ecosystem

### Documentation Goals
- âœ… Implementation roadmap (this document)
- âœ… API guide and examples
- âœ… Limitations and known issues documented
- âœ… Architecture and design docs
- ðŸš§ **TODO**: Comprehensive API reference (DocC)
- ðŸš§ **TODO**: Tutorial series (beginner to advanced)
- ðŸš§ **TODO**: Migration guides (from other Parquet libraries)

### Community Building
- ðŸš§ **TODO**: Public repository with clear contributing guide
- ðŸš§ **TODO**: Issue templates and PR guidelines
- ðŸš§ **TODO**: Benchmarking suite for transparency
- ðŸš§ **TODO**: Sample projects and use case demos

### Package Registry
- ðŸš§ **TODO**: Submit to Swift Package Index
- ðŸš§ **TODO**: Version tagging and release notes
- ðŸš§ **TODO**: Semantic versioning commitment

---

## Risk Assessment

### Technical Risks

1. **Writer complexity** (HIGH):
   - Writing is inherently more complex than reading
   - Level encoding bugs can produce corrupt files
   - Mitigation: Extensive testing, validation against Arrow C++

2. **Performance gaps** (MEDIUM):
   - Swift may not match C++ performance exactly
   - SIMD support differs across platforms
   - Mitigation: Profile early, optimize hot paths, set realistic expectations

3. **Platform quirks** (LOW):
   - File I/O differences on Linux/Windows
   - Endianness issues (unlikely but possible)
   - Mitigation: Cross-platform CI, test on all targets

### Ecosystem Risks

1. **Parquet format evolution** (LOW):
   - New encodings or page formats may be added
   - Breaking changes to format spec (very rare)
   - Mitigation: Monitor Apache Parquet releases, maintain forward compatibility

2. **Swift language changes** (LOW):
   - Swift 6 strict concurrency could require updates
   - Mitigation: Follow Swift evolution proposals, test with beta compilers

3. **Dependency stability** (VERY LOW):
   - snappy-swift is pure Swift and stable
   - No external C/C++ dependencies
   - Mitigation: Pin dependency versions, consider forking if needed

---

## Conclusion

**Phase 5 completion marks a major milestone**: Parquet-Swift now has a production-ready reader capable of handling virtually all real-world Parquet files with full nested type support.

**The path to 1.0 is clear**:
1. Implement core writer (Phase 10) - 8-10 weeks
2. Add Swift Concurrency support (Phase 17) - 3-4 weeks
3. Polish documentation and testing - 2-3 weeks
4. **Total: 3-4 months to 1.0 release**

**Post-1.0 priorities** will be driven by:
- User feedback and feature requests
- Performance profiling results
- Parquet ecosystem evolution
- Swift platform maturity

**The library is already useful today** for read-heavy workloads (data analysis, ETL, iOS/macOS data import). The writer will unlock the full potential for Swift-native data pipelines.

---

**Next Recommended Action**: Begin Phase 10 (Core Writer) design document and prototype basic file writing.
