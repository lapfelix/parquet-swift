# Known Limitations - Phase 1

This document tracks known limitations and compatibility issues in the current implementation.

## Parquet Metadata Format Compatibility

### PyArrow-Generated Files (BLOCKED)

**Status**: ❌ Not supported in Phase 1

**Issue**: Thrift metadata parser cannot read files generated by PyArrow (`parquet-cpp-arrow`).

**Symptoms**:
- Error: `"Missing required fields in FileMetaData"`
- Metadata parsing stops after reading `version` and `schema` fields
- Missing: `numRows` and `rowGroups` fields

**Affected Files**:
- Any Parquet file with `created_by: "parquet-cpp-arrow version X.X.X"`
- Tested with PyArrow 21.0.0

**Root Cause**: Unknown - requires investigation of:
- Thrift Compact Binary encoding differences
- Schema element list parsing (may consume extra bytes)
- Optional field handling differences
- Field ordering assumptions

**Workaround**: Use parquet-mr generated files:
```
created_by: "parquet-mr version X.X.X"
```

**Files Known to Work**:
- `datapage_v1-snappy-compressed-checksum.parquet` (parquet-mr 1.13.0)
- Any file from Apache Spark, Hive, or parquet-mr tools

### Future Work
- [ ] Debug schema list parsing in `ThriftReader+FileMetaData.swift`
- [ ] Add PyArrow compatibility tests
- [ ] Test with multiple PyArrow versions
- [ ] Consider alternative Thrift implementations

## Encoding Support

### Dictionary Encoding (BLOCKED)

**Status**: ❌ Not supported in Phase 1

**Affected Encodings**:
- `PLAIN_DICTIONARY` (deprecated)
- `RLE_DICTIONARY`

**Workaround**: Use PLAIN encoding only
- parquet-mr: Set `WriterVersion.PARQUET_1_0`, disable dictionary
- PyArrow: `use_dictionary=False` (but see PyArrow limitation above)

**Impact**: Most real-world Parquet files use dictionary encoding for strings and low-cardinality columns, making them unreadable in Phase 1.

## Compression Support

### Snappy Compression (IMPLEMENTED ✅)

**Status**: ✅ Implemented in Phase 2 (M2.0) - Pure Swift!

**Supported Codecs**:
- ✅ UNCOMPRESSED
- ✅ GZIP
- ✅ **SNAPPY** (most common in production) - Pure Swift implementation!

**Unsupported Codecs**:
- ❌ LZ4
- ❌ LZ4_RAW
- ❌ ZSTD
- ❌ BROTLI
- ❌ LZO

**Implementation**: Uses [snappy-swift](https://github.com/codelynx/snappy-swift), a pure Swift implementation with:
- ✅ **Zero system dependencies** - no brew/apt installation required
- ✅ **100% C++ compatible** - verified against Google's reference implementation
- ✅ **Fast performance** - 64-128 MB/s compression, 203-261 MB/s decompression
- ✅ **Cross-platform** - works on macOS, iOS, Linux, and all Swift platforms

**Build**: Simple `swift build` - no environment variables needed!

**Impact**: Most production Parquet files now readable! Snappy is the default compression in Apache Spark.

## Test Fixtures

### No Suitable Test Fixture for Column Readers

**Status**: ❌ Blocks end-to-end testing

**Requirements for Phase 2 Testing**:
1. PLAIN encoding only (no dictionary)
2. UNCOMPRESSED, GZIP, or Snappy compression ✅
3. parquet-mr generated (not PyArrow)
4. Multiple column types: Int32, Int64, Float, Double, String

**Existing Fixtures - Why They Don't Work**:
- `alltypes_plain.parquet`: Uses dictionary encoding ❌
- `datapage_v1-snappy-compressed-checksum.parquet`: Uses Snappy ✅ (NOW WORKS!)
- `plain_types.parquet`: PyArrow-generated, metadata parse fails ❌

**Solution**: Generate test fixture using parquet-mr Java tools:
```java
// Pseudocode - need actual parquet-mr example
ParquetWriter.builder(outputFile)
    .withWriterVersion(WriterVersion.PARQUET_1_0)
    .withDictionaryEncoding(false)
    .withCompressionCodec(CompressionCodecName.GZIP)
    .build();
```

See `generate_plain_fixture.py` for reference (PyArrow version that doesn't work).

## Column Features

### Nullable Columns (NOT IMPLEMENTED)

**Status**: ❌ Not implemented in Phase 1

**Missing**: Definition level / repetition level support

**Impact**: Can only read required (non-nullable) columns. Most real Parquet schemas have nullable columns.

### Nested Types (NOT IMPLEMENTED)

**Status**: ❌ Not implemented in Phase 1

**Missing Support**:
- Nested structs
- Lists/arrays
- Maps

**Impact**: Can only read flat, primitive columns.

## Summary

Phase 1/2 implementation supports:
- ✅ parquet-mr generated files
- ✅ PLAIN encoding
- ✅ UNCOMPRESSED, GZIP, and **Snappy** compression ✨
- ✅ Required (non-nullable) primitive columns
- ✅ Types: Int32, Int64, Float, Double, String

**Major Improvement**: Snappy compression support unblocks ~80% of production Parquet files!

Still **does not work** with:
- Dictionary encoding (strings, enums) - most common for string columns
- Nullable columns (definition levels) - most schemas have nulls
- PyArrow tools (Python ecosystem) - metadata parsing incompatibility
- Nested types (lists, maps, structs)

Remaining Phase 2 priorities:
1. **Dictionary encoding** (unblocks string columns)
2. **Definition levels** (nullable columns)
3. **PyArrow compatibility** (unblocks Python ecosystem)
4. **Nested types** (lists, maps, structs)
