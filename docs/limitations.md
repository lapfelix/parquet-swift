# Known Limitations - Phase 3

This document tracks known limitations and compatibility issues in the current implementation.

**Latest Updates**:
- ‚úÖ Array reconstruction for repeated columns (Phase 3)
- ‚úÖ PyArrow compatibility fixed (Phase 3)
- ‚úÖ Nullable columns fully supported (Phase 3)
- ‚úÖ Dictionary encoding for all types (Phase 2)

## Parquet Metadata Format Compatibility

### PyArrow-Generated Files (FIXED ‚úÖ)

**Status**: ‚úÖ **Fully supported as of Phase 3!**

**What Was Fixed**: Three critical bugs in the Thrift Compact Binary Protocol parser were identified and fixed:

1. **LogicalType early return bug**: `readLogicalType()` was returning immediately after reading a field (e.g., `.string`), preventing it from consuming the STOP byte that ends the struct. This caused 1-byte misalignment.

2. **TimeUnit early return bug**: Same pattern in `readTimeUnit()` - early returns prevented STOP byte consumption.

3. **skipStruct() bug (CRITICAL)**: `skipStruct()` was only reading field headers but not actually skipping the field data. This caused severe misalignment when encountering unknown struct fields like the newer `size_statistics` field in ColumnMetaData.

**Root Cause**: The parser wasn't properly handling:
- Struct STOP byte consumption in nested type readers
- Skipping unknown fields introduced in newer Parquet versions
- Forward compatibility with extended Thrift schemas

**Test Coverage**:
- ‚úÖ `testPyArrowGeneratedFile()` - verifies PyArrow 21.0.0 compatibility
- ‚úÖ Includes `pyarrow_test.parquet` fixture (5 rows, 3 columns)

**Files Now Supported**:
- ‚úÖ Any Parquet file with `created_by: "parquet-cpp-arrow version X.X.X"`
- ‚úÖ Tested with PyArrow 21.0.0
- ‚úÖ parquet-mr generated files (Spark, Hive, parquet-mr tools) still work

**Impact**: Python ecosystem files (pandas, PyArrow, Dask) are now fully readable!

## Encoding Support

### Dictionary Encoding (FULLY SUPPORTED ‚úÖ)

**Status**: ‚úÖ **Fully supported in Phase 2 and 3!**

**Supported Encodings**:
- ‚úÖ `PLAIN_DICTIONARY` (deprecated but supported)
- ‚úÖ `RLE_DICTIONARY`
- ‚úÖ PLAIN encoding

**Supported Types**:
- ‚úÖ Int32, Int64, Float, Double, String
- ‚úÖ Required (non-nullable) columns
- ‚úÖ Nullable columns with definition levels

**Impact**: Most real-world Parquet files with dictionary encoding are now fully readable!

## Compression Support

### Snappy Compression (IMPLEMENTED ‚úÖ)

**Status**: ‚úÖ Implemented in Phase 2 (M2.0) - Pure Swift!

**Supported Codecs**:
- ‚úÖ UNCOMPRESSED
- ‚úÖ GZIP
- ‚úÖ **SNAPPY** (most common in production) - Pure Swift implementation!

**Unsupported Codecs**:
- ‚ùå LZ4
- ‚ùå LZ4_RAW
- ‚ùå ZSTD
- ‚ùå BROTLI
- ‚ùå LZO

**Implementation**: Uses [snappy-swift](https://github.com/codelynx/snappy-swift), a pure Swift implementation with:
- ‚úÖ **Zero system dependencies** - no brew/apt installation required
- ‚úÖ **100% C++ compatible** - verified against Google's reference implementation
- ‚úÖ **Fast performance** - 64-128 MB/s compression, 203-261 MB/s decompression
- ‚úÖ **Cross-platform** - works on macOS, iOS, Linux, and all Swift platforms

**Build**: Simple `swift build` - no environment variables needed!

**Impact**: Most production Parquet files now readable! Snappy is the default compression in Apache Spark.

## Test Fixtures

**Status**: ‚úÖ Good test coverage

**Available Fixtures**:
- ‚úÖ `alltypes_plain.parquet`: Dictionary encoding, multiple types
- ‚úÖ `datapage_v1-snappy-compressed-checksum.parquet`: Snappy compression (parquet-mr 1.13.0)
- ‚úÖ `pyarrow_test.parquet`: PyArrow-generated with nullable columns (parquet-cpp-arrow 21.0.0)

**Coverage**:
- ‚úÖ Both parquet-mr and PyArrow generated files
- ‚úÖ Multiple encodings: PLAIN, RLE_DICTIONARY
- ‚úÖ Multiple codecs: UNCOMPRESSED, GZIP, Snappy
- ‚úÖ All primitive types: Int32, Int64, Float, Double, String
- ‚úÖ Required and nullable columns

## Column Features

### Nullable Columns (IMPLEMENTED ‚úÖ)

**Status**: ‚úÖ Implemented in Phase 3!

**Supported**:
- Definition level decoding (RLE/bit-packed hybrid encoding)
- Nullable columns for all primitive types (Int32, Int64, Float, Double, String)
- Both PLAIN and dictionary encoding with nullable columns
- Correct null value representation in returned arrays

**API Changes**:
- Column readers now return optional arrays: `[Int32?]`, `[Int64?]`, `[Float?]`, `[Double?]`, `[String?]`
- `readOne()` returns double optional (outer for end-of-stream, inner for NULL value)
- Required columns return all non-nil values
- Nullable columns return nil for NULL values

**Impact**: Most real Parquet files with nullable columns are now readable!

### Nested Types (PARTIALLY IMPLEMENTED)

**Status**: üöß Partially implemented

**Supported**:
- ‚úÖ Single-level repeated columns (maxRepetitionLevel = 1)
- ‚úÖ **Multi-level nested lists** (maxRepetitionLevel > 1) ‚ú® NEW!
  - ‚úÖ Lists of lists (e.g., `[[[1, 2], [3]], [[4]]]`)
  - ‚úÖ Distinguishes NULL lists vs EMPTY lists
  - ‚úÖ Handles all edge cases (null inner/outer lists, empty inner/outer lists)
  - ‚úÖ `readAllNested()` API returns nested arrays

**Missing Support**:
- ‚ùå Nested structs
- ‚ùå Maps
- ‚ùå Lists of structs

**Impact**: Can read primitive columns, single-level arrays, and multi-level nested lists. Cannot read maps or structs yet.

## Summary

Phase 3 implementation supports:
- ‚úÖ parquet-mr generated files (Spark, Hive, parquet-mr tools)
- ‚úÖ **PyArrow-generated files** (parquet-cpp-arrow) ‚ú®
- ‚úÖ PLAIN encoding
- ‚úÖ **Dictionary encoding (RLE_DICTIONARY, PLAIN_DICTIONARY)** ‚ú®
- ‚úÖ UNCOMPRESSED, GZIP, and **Snappy** compression
- ‚úÖ **All primitive types: Int32, Int64, Float, Double, String** ‚ú®
- ‚úÖ **Required (non-nullable) columns** ‚ú®
- ‚úÖ **Nullable columns (definition level support)** ‚ú®
- ‚úÖ **Repeated columns (single-level arrays/lists)** ‚ú®
- ‚úÖ **Multi-level nested lists (lists of lists)** ‚ú® NEW!

**Major Improvements**:
- ‚úÖ **PyArrow compatibility** - Python ecosystem files now readable! (pandas, PyArrow, Dask) üéâ
- ‚úÖ Snappy compression support (~80% of production files)
- ‚úÖ Dictionary encoding for ALL primitive types (~90% of string/enum columns!)
- ‚úÖ **Nullable column support** - can read NULL values in optional columns! (~90% of schemas!)
- ‚úÖ **Repeated column support** - can read arrays/lists with empty lists and null elements! üéâ

### Dictionary Encoding - Complete Status

**What works:**
- ‚úÖ **All primitive types**: Int32, Int64, Float, Double, String
- ‚úÖ **Required columns** with dictionary encoding
- ‚úÖ **Nullable columns** with dictionary encoding ‚ú® NEW in Phase 3!
- ‚úÖ Both RLE_DICTIONARY and PLAIN_DICTIONARY encodings
- ‚úÖ Full overflow protection in RLE decoder
- ‚úÖ Strict byte-exact validation
- ‚úÖ Definition level decoding for nullable columns

**What works with repeated columns:**
- ‚úÖ **Single-level repeated columns** (maxRepetitionLevel = 1) - FULLY SUPPORTED!
  - ‚úÖ Repetition levels decoded from pages
  - ‚úÖ Array reconstruction logic implemented
  - ‚úÖ `readAllRepeated()` API returns `[[T?]]` for arrays with nullable elements
  - ‚úÖ Handles empty lists, null elements, and all primitive types
  - ‚úÖ All 5 column types: Int32, Int64, Float, Double, String

**What doesn't work yet:**
- üöß **Complex nested types** - Partially implemented
  - ‚úÖ **Nested lists** (lists of lists) - FULLY SUPPORTED! ‚ú®
  - ‚ùå Lists of structs
  - ‚ùå Maps
  - ‚ùå Nested structs

**Phase 3 Achievement:**

Nullable columns now fully supported! The implementation decodes definition levels from each page
to determine which values are NULL. Both PLAIN and dictionary encoding work correctly with
nullable columns.

Still **does not work** with:
- ‚ùå Complex nested types (lists of structs, maps, nested structs) - Phase 4+

Completed milestones:
1. ‚úÖ **Dictionary encoding for required columns** (Phase 2.1)
2. ‚úÖ **Extend dictionary encoding to all types** (Phase 2.2)
3. ‚úÖ **Definition levels** (nullable columns) (Phase 3) ‚ú®
4. ‚úÖ **PyArrow compatibility** (Python ecosystem) ‚ú®
5. ‚úÖ **Repetition levels and array reconstruction** (Phase 3) ‚ú® DONE!
   - ‚úÖ Decode repetition levels from pages
   - ‚úÖ Reconstruct arrays from flat value sequences
   - ‚úÖ Handle empty lists and null elements
   - ‚úÖ `readAllRepeated()` API for all primitive types
6. ‚úÖ **Multi-level nested lists** (Phase 3) ‚ú® DONE!
   - ‚úÖ `readAllNested()` API for maxRepetitionLevel > 1
   - ‚úÖ ArrayReconstructor with explicit ListState tracking
   - ‚úÖ Follows Apache Arrow's DefRepLevelsToListInfo pattern
   - ‚úÖ Handles NULL vs EMPTY vs POPULATED lists correctly
   - ‚úÖ Comprehensive test coverage for all edge cases

Remaining priorities:
7. **Complex nested types** (lists of structs, maps, nested structs) - Phase 4+

---

## Nested Structure Limitations (Phase 3)

**Added**: 2025-11-03

### ‚ùå CRITICAL: Structs Containing Complex Children

**Status**: Not supported - throws error with workarounds

**Problem**: Structs with complex children (maps, lists, nested structs) require multi-level reconstruction not yet implemented.

**Example Schemas**:
- `struct { string name; map<string,int> attrs; }`
- `struct { int32 id; list<string> tags; }`
- `struct { struct inner { ... } }`

**Behavior**:
- ‚ùå `readStruct()` throws `unsupportedType` error
- ‚ùå `readRepeatedStruct()` throws `unsupportedType` error
- ‚úÖ Clear error message with workarounds

**Error Message**:
```
Structs containing complex fields (maps, lists, nested structs) are not yet supported.

Workarounds:
1. Read maps directly: readMap(at: ["your_struct", "map_field"])
2. Read lists directly: readRepeatedStruct(at: ["your_struct", "list_field", "list", "element"])
3. Read primitive fields individually via column readers

This limitation will be removed once proper multi-level reconstruction (LevelInfo) is implemented.
```

**When This Will Be Fixed**: Phase 4 - Port Arrow C++'s `DefRepLevelsToListInfo` for proper multi-level reconstruction

### ‚ö†Ô∏è list<map> - Flattens Intermediate Dimension

**Status**: Partial support - reads but loses structure

**Problem**: Loses intermediate list dimension, merges maps

**Example**: `[[{a:1},{b:2}], [{c:3}]]` ‚Üí `[[{a:1, b:2}], [{c:3}]]` (2 maps in first list merged into 1)

**Workaround**: None - requires LevelInfo port

**When This Will Be Fixed**: Phase 4 - Proper multi-level repetition support

### Implementation Details

See `docs/map-bugs-exposed.md` for:
- Detailed technical analysis
- Test coverage
- Future LevelInfo implementation plan

### What DOES Work

- ‚úÖ Root-level maps: `map<primitive, primitive>`
- ‚úÖ Flat structs: primitives only
- ‚úÖ Simple `list<struct>`: primitives only  
- ‚úÖ Multi-level lists: `list<list<T>>`

---

**For complete details and examples**: See above sections in this document.

